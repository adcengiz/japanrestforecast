{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n",
    "import scipy.io as scipio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read train & test data\n",
    "data_train = pd.DataFrame(pd.read_csv(\"data_train.csv\"))\n",
    "data_test = pd.DataFrame(pd.read_csv(\"data_test.csv\"))\n",
    "data_train = data_train.dropna(0)\n",
    "data_test = data_test.dropna(0)\n",
    "\n",
    "## index by visit_date column \n",
    "## the dates are not unique yet\n",
    "data_train = data_train.set_index(\"visit_date\", drop=True)\n",
    "data_test = data_test.set_index(\"visit_date\", drop=True)\n",
    "\n",
    "## convert the dates from string to datetime\n",
    "arima_dates_train = pd.Series(data_train.index.values).\\\n",
    "apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "arima_dates_test = pd.Series(data_test.index.values).\\\n",
    "apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "\n",
    "## get the unique date index for full data\n",
    "data = pd.concat([data_train,data_test],0)\n",
    "arima_dates_full = pd.Series(data.index.values).\\\n",
    "apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "## leave out Feb 2017 data for each restaurant df \n",
    "## as validation split\n",
    "def train_val_split(path):\n",
    "    \"\"\"Takes as input a directory, splits all the training csv \n",
    "    files in the directory as training and validation, appends the \n",
    "    dataframes to separate dictionaries, where keys are rest ids\"\"\"\n",
    "    train_dict = {}\n",
    "    val_dict = {}\n",
    "    for rest_file in os.listdir(path)[1:]:\n",
    "        data = pd.DataFrame(pd.read_csv(path + rest_file))\n",
    "        data = data.set_index(\"visit_date\",drop=True)\n",
    "        ## convert index to datetime to filter\n",
    "        data.index = pd.Series(data.index).\\\n",
    "        apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "        ## filter the records before 2017-02-01 for training\n",
    "        data_train = data[data.index<datetime.strptime(\"2017-02-01\", \"%Y-%m-%d\")]\n",
    "        ## filter the records after 2017-02-01 for validation\n",
    "        data_val = data[data.index>=datetime.strptime(\"2017-02-01\", \"%Y-%m-%d\")]\n",
    "        ## return two datasets: training and validation\n",
    "        train_dict[rest_file] = data_train\n",
    "        val_dict[rest_file] = data_val\n",
    "    return train_dict, val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"arima_rest_data/train/\"\n",
    "train_dict, val_dict = train_val_split(path)[0], train_val_split(path)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endog_col = [\"visitors\"]\n",
    "exog_col = ['Friday', 'Monday', 'Saturday','Sunday', 'Thursday', \n",
    "            'Tuesday', 'Wednesday','avg_temperature1','high_temperature1', \n",
    "            'holiday_flg', 'hours_sunlight1','low_temperature1',\n",
    "           'total_area_visitors', 'total_genre_visitors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## manual param grid\n",
    "import itertools\n",
    "\n",
    "order_p = np.array([1,7])\n",
    "order_d = np.arange(1,3) ## order of differencing\n",
    "order_q = np.array([1,7])\n",
    "s_order_p = np.array([1,7])\n",
    "s_order_d = np.arange(1,3) ## order of seasonal differencing\n",
    "s_order_q = np.array([1,7])\n",
    "s_order_s = np.array([4,1]) ## seasons per year\n",
    "param_trend = [\"n\",\"c\"]\n",
    "\n",
    "grid_lists = [order_p, order_d, order_q, s_order_p, s_order_d,\\\n",
    "             s_order_q, s_order_s, param_trend]\n",
    "\n",
    "grid = list(itertools.product(*grid_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endog_col = [\"visitors\"]\n",
    "exog_col = ['Friday', 'Monday', 'Saturday','Sunday', 'Thursday', \n",
    "            'Tuesday', 'Wednesday','avg_temperature1','high_temperature1', \n",
    "            'holiday_flg', 'hours_sunlight1','low_temperature1',\n",
    "           'total_area_visitors', 'total_genre_visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = list(itertools.product(*grid_lists))\n",
    "## delete from directory\n",
    "# air_28dbe91c4c9656be.csv\n",
    "# air_2a485b92210c98b5.csv\n",
    "\n",
    "# grid_model_params = {}\n",
    "for params in grid:\n",
    "    print (params)\n",
    "    ## will average the learned parameters of all \n",
    "    ## the restaurants for every set of parameter \n",
    "    ## then will pass them to the prediction function\n",
    "    ## as start params\n",
    "    grid_model_params[params] = {}\n",
    "    ## select 150 random restaurants\n",
    "    for rest in [os.listdir(\"arima_rest_data/train/\")[1:][ix] for ix in \\\n",
    "                 [np.random.choice(665) for x in range(10)]]:\n",
    "        print (str(rest))\n",
    "        ## train_dict: previously written training dictionary of all rests \n",
    "        data = train_dict[rest]\n",
    "        if \"total_area_visitors\" not in data.columns.values or \"total_genre_visitors\" not in data.columns.values:\n",
    "            None\n",
    "        elif data[exog_col].shape[0]<2 or data[exog_col].shape[1]<8\\\n",
    "        or len(data[endog_col])<8: \n",
    "            None\n",
    "        ## params_exog = np.linalg.pinv(exog).dot(endog)\n",
    "        elif data[endog_col].shape[0] < 1 or data[endog_col].shape[1] < 1:\n",
    "            None\n",
    "        elif len((np.linalg.pinv(data[exog_col]).dot(data[endog_col])).shape) < 2: \n",
    "            None\n",
    "        # endog = endog - np.dot(exog, params_exog)\n",
    "        elif np.dot(data[exog_col], np.linalg.pinv(data[exog_col]).dot(data[endog_col])).shape[0]<1\\\n",
    "        or np.dot(data[exog_col], np.linalg.pinv(data[exog_col]).dot(data[endog_col])).shape[1]<1:\n",
    "            None\n",
    "        elif rest == \"air_28dbe91c4c9656be.csv\":\n",
    "            None\n",
    "        elif rest == \"air_4dea8d17f6f59c56.csv\":\n",
    "            None\n",
    "        elif rest == \".ipynb_checkpoints\":\n",
    "            None\n",
    "        else:\n",
    "            sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                           order=(int(params[0]),int(params[1]),int(params[2])),seasonal_order=(int(params[3]),int(params[4]),int(params[5]),int(params[6])),\\\n",
    "                                    trend=params[7], mle_regression=True, enforce_invertibility=False)\n",
    "\n",
    "            grid_model_params[params][rest] = sarimax_model.fit(maxiter=1000,disp=True,return_params=True,full_output=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_mape(true, pred):\n",
    "    \"\"\"args:\n",
    "    - true: the true array of \"visitor\" numbers \n",
    "            for the determined timeframe - shape: (n_days,)\n",
    "    - pred: the predicted array of \"visitor\" numbers\n",
    "            for the determined timeframe - shape: (n_days,)\n",
    "       returns:\n",
    "    - avg_percent_error: average percentage error in terms\n",
    "                        of visitors\"\"\"\n",
    "    abs_error = np.abs(np.subtract(true,pred))\n",
    "    loss = np.divide(abs_error,true)\n",
    "    avg_percent_error = np.mean(loss)\n",
    "    return avg_percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_squared(true, pred):\n",
    "    \"\"\"args:\n",
    "    - true: the true array of \"visitor\" numbers \n",
    "            for the determined timeframe - shape: (n_days,)\n",
    "    - pred: the predicted array of \"visitor\" numbers\n",
    "            for the determined timeframe - shape: (n_days,)\n",
    "       returns:\n",
    "    - avg_square_error: average squared error in terms\n",
    "                        of visitors, averaged by the number\n",
    "                        of days\"\"\"\n",
    "    num_days = len(true)\n",
    "    sq_error = np.square(np.array(true-pred))\n",
    "    avg_square_error = (1/num_days)*np.sum(sq_error)\n",
    "    return avg_square_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_iters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SARIMAXGridSearch(object):\n",
    "    def __init__(self, param_grid=grid, rest_dir=\"arima_rest_data/train/\",\\\n",
    "                 num_rests=10, len_rests=600):\n",
    "        \n",
    "        self.param_grid = param_grid\n",
    "        self.rest_dir = rest_dir\n",
    "        self.num_rests = num_rests\n",
    "        self.len_rests = len_rests\n",
    "        \n",
    "    def select_setting(self, grid_index):\n",
    "        \n",
    "        ## n = int index of param_grid\n",
    "        param_grid = self.param_grid\n",
    "        rest_dir = self.rest_dir\n",
    "        num_rests = self.num_rests\n",
    "        len_rests = self.len_rests\n",
    "        ## grid settings\n",
    "        params = param_grid[grid_index]\n",
    "        rests = [os.listdir(rest_dir)[1:][ix] for ix in \\\n",
    "                 [np.random.choice(len_rests) for x in range(num_rests)]]\n",
    "        \n",
    "        self.params = params\n",
    "        self.rests = rests\n",
    "        return self\n",
    "    \n",
    "    def averaging_fit(self):\n",
    "        params = self.params\n",
    "        rests = self.rests\n",
    "        model_params = {}\n",
    "        \n",
    "        if len(rests) == 1:\n",
    "            if \"total_area_visitors\" not in train_dict[rests[0]].columns.values or\\\n",
    "            \"total_genre_visitors\" not in train_dict[rests[0]].columns.values:\n",
    "                pass\n",
    "            elif train_dict[rests[0]].shape[0] < 24 and val_dict[rests[0]].shape[0] < 8:\n",
    "                pass\n",
    "            else:\n",
    "                data = train_dict[rest]\n",
    "                val_data = val_dict[rest]\n",
    "                sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                                   order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                            seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                            int(params[5]),int(params[6])),\\\n",
    "                                            trend=params[7], mle_regression=True, \\\n",
    "                                            enforce_invertibility=False)\n",
    "                results_object = sarimax_model.fit(maxiter=1000,method=\"bfgs\")\n",
    "                self.results_object = results_object\n",
    "\n",
    "        else:\n",
    "            rest_params = []\n",
    "            for rest_ix in range(len(rests)):\n",
    "                if \"total_area_visitors\" not in train_dict[rests[rest_ix]].columns.values or\\\n",
    "                \"total_genre_visitors\" not in train_dict[rests[rest_ix]].columns.values:\n",
    "                    continue\n",
    "                elif train_dict[rests[rest_ix]].shape[0] < 24 and val_dict[rests[rest_ix]].shape[0] < 24:\n",
    "                    continue\n",
    "                else:\n",
    "                    data = train_dict[rests[rest_ix]]\n",
    "                    sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                                   order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                            seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                            int(params[5]),int(params[6])),\\\n",
    "                                            trend=params[7], mle_regression=True, \\\n",
    "                                            enforce_invertibility=False)\n",
    "                    results_object = sarimax_model.fit(start_params=None,maxiter=1000,method=\"bfgs\",\\\n",
    "                                                          return_params=True)\n",
    "                    rest_params.append(results_object)\n",
    "                    final_start_params = np.mean(rest_params,0)\n",
    "            self.average_params = final_start_params\n",
    "        return self\n",
    "    \n",
    "    def bag_pred(self):\n",
    "        rests = self.rests\n",
    "        params = self.params\n",
    "        average_params = self.average_params\n",
    "        rest_preds = {}\n",
    "        for rest in rests:\n",
    "            data = train_dict[rest]\n",
    "            val_data = val_dict[rest]\n",
    "            print (rest)\n",
    "            sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                               order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                        seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                        int(params[5]),int(params[6])),\\\n",
    "                                        trend=params[7], mle_regression=True, \\\n",
    "                                        enforce_invertibility=False,time_varying_regression=False)\n",
    "            final_model = sarimax_model.fit(start_params=average_params,\\\n",
    "                                              maxiter=1000,method=\"bfgs\")\n",
    "            bag_preds_ = final_model.predict\\\n",
    "                    (start=len(data)-1, end=len(data)+len(val_data)-1,dynamic=True,\\\n",
    "                    exog=val_data[exog_col])\n",
    "            rest_preds[rest] = bag_preds_\n",
    "            \n",
    "        self.preds = rest_preds\n",
    "        return self, rest_preds\n",
    "    \n",
    "    def bag_losses(self):\n",
    "        rests = self.rests\n",
    "        params = self.params\n",
    "        average_params = self.average_params\n",
    "        preds = self.preds\n",
    "        bagged_rest_scores = {}\n",
    "        for rest in self.rests:\n",
    "            val_data = val_dict[rest]\n",
    "            pred = preds[rest]\n",
    "            true = np.array(val_data[\"visitors\"])\n",
    "            abs_percent_loss = loss_mape(true,pred[:len(true)])\n",
    "            square_loss = loss_squared(true,pred[:len(true)])\n",
    "            bagged_rest_scores[rest] = [abs_percent_loss,square_loss] \n",
    "        return bagged_rest_scores\n",
    "    \n",
    "    def iterative_fit(self):\n",
    "        ### param yazdirabildigimize gore, ortalama almali yapabilriiz\n",
    "        ## !!!!!!!!!!! yay \n",
    "        ## bir de tekrar fit etmemeli yazmaya calis\n",
    "        params = self.params\n",
    "        rests = self.rests\n",
    "        model_params = {}\n",
    "        \n",
    "        if len(rests) == 1:\n",
    "            data = train_dict[rest]\n",
    "            val_data = val_dict[rest]\n",
    "            sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                               order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                        seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                        int(params[5]),int(params[6])),\\\n",
    "                                        trend=params[7], mle_regression=True, \\\n",
    "                                        enforce_invertibility=False)\n",
    "            \n",
    "            results_object = sarimax_model.fit(maxiter=1000,method=\"bfgs\")\n",
    "            self.results_object = results_object\n",
    "        else:\n",
    "            for rest_ix in range(len(rests)):\n",
    "                if \"total_area_visitors\" not in train_dict[rests[rest_ix]].columns.values or\\\n",
    "                \"total_genre_visitors\" not in train_dict[rests[rest_ix]].columns.values:\n",
    "                    continue\n",
    "                elif train_dict[rests[rest_ix]].shape[0] < 24 and val_dict[rests[rest_ix]].shape[0] < 24:\n",
    "                    continue\n",
    "                else:\n",
    "                    data = train_dict[rests[rest_ix]]\n",
    "                    val_data = val_dict[rests[rest_ix]]\n",
    "                    sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                                   order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                            seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                            int(params[5]),int(params[6])),\\\n",
    "                                            trend=params[7], mle_regression=True, \\\n",
    "                                            enforce_invertibility=False)\n",
    "                    if rest_ix == 0:\n",
    "                        results_object = sarimax_model.fit(start_params=None,maxiter=1000,method=\"bfgs\",\\\n",
    "                                                          return_params=True)\n",
    "                        model_params[rests[rest_ix]] = results_object\n",
    "                    elif rest_ix != 0 and rest_ix < len(rests)-1:\n",
    "                        results_object = sarimax_model.fit(start_params=model_params[rests[rest_ix-1]],maxiter=1000,method=\"bfgs\",\\\n",
    "                                                          return_params=True)\n",
    "                        model_params[rests[rest_ix]] = results_object\n",
    "                    elif rest_ix == len(rests)-1:\n",
    "                        results_object = sarimax_model.fit(start_params=model_params[rests[rest_ix-1]],\\\n",
    "                                                           maxiter=1000,method=\"bfgs\",return_params=True)\n",
    "                        model_params[rests[rest_ix]] = results_object\n",
    "            self.results_object = model_params[rests[len(rests)-1]]\n",
    "        return self\n",
    "    \n",
    "    def final_preds(self):\n",
    "        rests = self.rests\n",
    "        params = self.params\n",
    "        results_object = self.results_object\n",
    "        \n",
    "        rest_preds = {}\n",
    "        for rest in rests:\n",
    "            if \"total_area_visitors\" not in train_dict[rest].columns.values or\\\n",
    "                \"total_genre_visitors\" not in train_dict[rest].columns.values:\n",
    "                continue\n",
    "            elif train_dict[rest].shape[0] < 24 and val_dict[rest].shape[0] < 24:\n",
    "                continue\n",
    "            else:\n",
    "                data = train_dict[rest]\n",
    "                val_data = val_dict[rest]\n",
    "                sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                                   order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                            seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                            int(params[5]),int(params[6])),\\\n",
    "                                            trend=params[7], mle_regression=True, \\\n",
    "                                            enforce_invertibility=False,time_varying_regression=False)\n",
    "                final_model = sarimax_model.fit(start_params=results_object,\\\n",
    "                                                  maxiter=1000,method=\"bfgs\")\n",
    "                preds_ = final_model.predict\\\n",
    "                        (start=len(data)-1, end=len(data)+len(val_data)-1,dynamic=True,\\\n",
    "                        exog=val_data[exog_col])\n",
    "                rest_preds[rest] = preds_\n",
    "\n",
    "        self.preds = rest_preds\n",
    "        return self, rest_preds\n",
    "    \n",
    "    def final_scores(self):\n",
    "        preds = self.preds\n",
    "#         print (preds)\n",
    "        rests = self.rests\n",
    "#         print (rests)\n",
    "        rest_scores = {}\n",
    "        for rest in rests:\n",
    "            val_data = val_dict[rest]\n",
    "            pred = preds[rest]\n",
    "            true = np.array(val_data[\"visitors\"])\n",
    "            abs_percent_loss = loss_mape(true,pred[:len(true)])\n",
    "            square_loss = loss_squared(true,pred[:len(true)])\n",
    "            rest_scores[rest] = [abs_percent_loss,square_loss]\n",
    "        return rest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {}\n",
    "grid_try = [(1,1,1,1,1,1,1,\"c\")]\n",
    "for p in range(len(grid_try)):\n",
    "    print (grid_try[p])\n",
    "    gsearch = SARIMAXGridSearch(param_grid=grid_try, rest_dir=\"arima_rest_data/train/\",\\\n",
    "                 num_rests=100, len_rests=600)\n",
    "    gsearch = gsearch.select_setting(p)\n",
    "    gsearch.iterative_fit()\n",
    "    gsearch.final_preds()\n",
    "    grid_search[grid_try[p]] = gsearch.final_scores()\n",
    "# grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search[(1,1,1,1,1,1,1,\"c\")]).to_csv(\"grid_search_preds/scores/iterative/(1,1,1,1,1,1,1,c)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch = SARIMAXGridSearch(param_grid=grid, rest_dir=\"arima_rest_data/train/\",\\\n",
    "                 num_rests=15, len_rests=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsearch = gsearch.select_setting(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.418568\n",
      "         Iterations: 229\n",
      "         Function evaluations: 244\n",
      "         Gradient evaluations: 244\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.988528\n",
      "         Iterations: 199\n",
      "         Function evaluations: 221\n",
      "         Gradient evaluations: 221\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.075766\n",
      "         Iterations: 245\n",
      "         Function evaluations: 264\n",
      "         Gradient evaluations: 264\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.669097\n",
      "         Iterations: 89\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.870229\n",
      "         Iterations: 335\n",
      "         Function evaluations: 484\n",
      "         Gradient evaluations: 473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.700973\n",
      "         Iterations: 236\n",
      "         Function evaluations: 255\n",
      "         Gradient evaluations: 255\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.969288\n",
      "         Iterations: 209\n",
      "         Function evaluations: 297\n",
      "         Gradient evaluations: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.613834\n",
      "         Iterations: 190\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 199\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.217192\n",
      "         Iterations: 140\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 154\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.040285\n",
      "         Iterations: 414\n",
      "         Function evaluations: 530\n",
      "         Gradient evaluations: 521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.563948\n",
      "         Iterations: 321\n",
      "         Function evaluations: 410\n",
      "         Gradient evaluations: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.531848\n",
      "         Iterations: 214\n",
      "         Function evaluations: 231\n",
      "         Gradient evaluations: 231\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.324684\n",
      "         Iterations: 203\n",
      "         Function evaluations: 218\n",
      "         Gradient evaluations: 218\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.703693\n",
      "         Iterations: 244\n",
      "         Function evaluations: 286\n",
      "         Gradient evaluations: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SARIMAXGridSearch at 0x1c1abfd198>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.averaging_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_735bcbe1763d6e98.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.418581\n",
      "         Iterations: 226\n",
      "         Function evaluations: 237\n",
      "         Gradient evaluations: 237\n",
      "air_35512c42db0868da.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.988528\n",
      "         Iterations: 160\n",
      "         Function evaluations: 169\n",
      "         Gradient evaluations: 169\n",
      "air_b45b8e456f53942a.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.067858\n",
      "         Iterations: 185\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 193\n",
      "air_56ea46c14b2dd967.csv\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.667021\n",
      "         Iterations: 248\n",
      "         Function evaluations: 354\n",
      "         Gradient evaluations: 342\n",
      "air_3bb99a1fe0583897.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.869868\n",
      "         Iterations: 242\n",
      "         Function evaluations: 263\n",
      "         Gradient evaluations: 263\n",
      "air_dea0655f96947922.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.700973\n",
      "         Iterations: 231\n",
      "         Function evaluations: 263\n",
      "         Gradient evaluations: 263\n",
      "air_d4d218b451f82c3d.csv\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.987064\n",
      "         Iterations: 237\n",
      "         Function evaluations: 274\n",
      "         Gradient evaluations: 264\n",
      "air_8093d0b565e9dbdf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.613483\n",
      "         Iterations: 473\n",
      "         Function evaluations: 525\n",
      "         Gradient evaluations: 525\n",
      "air_caf996ac27206301.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.214641\n",
      "         Iterations: 141\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 151\n",
      "air_465bddfed3353b23.csv\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.042214\n",
      "         Iterations: 364\n",
      "         Function evaluations: 632\n",
      "         Gradient evaluations: 612\n",
      "air_399904bdb7685ca0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.564443\n",
      "         Iterations: 269\n",
      "         Function evaluations: 379\n",
      "         Gradient evaluations: 368\n",
      "air_9aa92007e3628dbc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.531059\n",
      "         Iterations: 251\n",
      "         Function evaluations: 313\n",
      "         Gradient evaluations: 301\n",
      "air_9c6787aa03a45586.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 31\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 118\n",
      "air_c3585b0fba3998d0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.324684\n",
      "         Iterations: 194\n",
      "         Function evaluations: 215\n",
      "         Gradient evaluations: 213\n",
      "air_97b2a9f975fc702c.csv\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.718183\n",
      "         Iterations: 307\n",
      "         Function evaluations: 330\n",
      "         Gradient evaluations: 330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.SARIMAXGridSearch at 0x1c1abfd198>,\n",
       " {'air_35512c42db0868da.csv': 337     6.219898\n",
       "  338     7.491017\n",
       "  339     7.662160\n",
       "  340     8.966549\n",
       "  341    10.885319\n",
       "  342    10.513021\n",
       "  343     5.674808\n",
       "  344     6.363556\n",
       "  345     8.301537\n",
       "  346     6.454974\n",
       "  347     8.572288\n",
       "  348    12.973392\n",
       "  349    10.355408\n",
       "  350     6.220201\n",
       "  351     6.506216\n",
       "  352     8.185356\n",
       "  353     7.337406\n",
       "  354     8.706348\n",
       "  355    11.582749\n",
       "  356    11.119042\n",
       "  357     5.184369\n",
       "  358     7.130954\n",
       "  359     8.078373\n",
       "  360     7.374720\n",
       "  361     9.962675\n",
       "  362    11.462032\n",
       "  363    11.267487\n",
       "  364     6.682769\n",
       "  365     7.085599\n",
       "  dtype: float64, 'air_399904bdb7685ca0.csv': 378    21.434092\n",
       "  379    35.863454\n",
       "  380    16.832199\n",
       "  381    48.824591\n",
       "  382    39.443823\n",
       "  383    31.212278\n",
       "  384    12.386226\n",
       "  385    19.694704\n",
       "  386    25.350725\n",
       "  387    24.337405\n",
       "  388    46.250057\n",
       "  389    54.823268\n",
       "  390    34.688540\n",
       "  391    18.268206\n",
       "  392    22.509926\n",
       "  393    22.050580\n",
       "  394    22.070123\n",
       "  395    44.112543\n",
       "  396    34.033679\n",
       "  397    21.420745\n",
       "  398    18.555398\n",
       "  399    13.416821\n",
       "  400    27.167360\n",
       "  401    16.829302\n",
       "  402    40.923498\n",
       "  403    34.497124\n",
       "  404    21.844148\n",
       "  405     0.712220\n",
       "  406    13.230136\n",
       "  dtype: float64, 'air_3bb99a1fe0583897.csv': 382    33.434527\n",
       "  383    39.916875\n",
       "  384    35.145813\n",
       "  385    54.987080\n",
       "  386    46.246191\n",
       "  387    30.836841\n",
       "  388    32.884532\n",
       "  389    42.051433\n",
       "  390    33.905225\n",
       "  391    53.427308\n",
       "  392    38.650038\n",
       "  393    31.907153\n",
       "  394    34.663445\n",
       "  395    43.742270\n",
       "  396    39.958404\n",
       "  397    54.715700\n",
       "  398    47.738033\n",
       "  399    24.468777\n",
       "  400    34.054223\n",
       "  401    35.647408\n",
       "  402    43.628800\n",
       "  403    40.504619\n",
       "  404    50.800616\n",
       "  405    52.066931\n",
       "  406    30.723318\n",
       "  407    37.688787\n",
       "  dtype: float64, 'air_465bddfed3353b23.csv': 209    34.151626\n",
       "  210    36.909291\n",
       "  211    32.458801\n",
       "  212    41.328212\n",
       "  213    52.740013\n",
       "  214    34.521553\n",
       "  215    31.786537\n",
       "  216    34.701234\n",
       "  217    36.773006\n",
       "  218    32.413995\n",
       "  219    45.953054\n",
       "  220    50.286749\n",
       "  221    40.343875\n",
       "  222    33.583714\n",
       "  223    37.064224\n",
       "  224    39.999707\n",
       "  225    38.606502\n",
       "  226    44.555768\n",
       "  227    53.074124\n",
       "  228    41.984146\n",
       "  229    31.670844\n",
       "  230    38.244313\n",
       "  231    36.651453\n",
       "  232    35.071214\n",
       "  233    48.203037\n",
       "  234    57.417293\n",
       "  235    43.048498\n",
       "  236    35.342487\n",
       "  237    40.017486\n",
       "  dtype: float64, 'air_56ea46c14b2dd967.csv': 199    33.590902\n",
       "  200    33.379621\n",
       "  201    30.279766\n",
       "  202    40.232146\n",
       "  203    48.648435\n",
       "  204    41.455280\n",
       "  205    31.145557\n",
       "  206    35.027486\n",
       "  207    37.212949\n",
       "  208    25.703404\n",
       "  209    35.565166\n",
       "  210    52.947631\n",
       "  211    42.704027\n",
       "  212    34.781396\n",
       "  213    35.095147\n",
       "  214    37.771508\n",
       "  215    35.227557\n",
       "  216    44.792029\n",
       "  217    49.061654\n",
       "  218    45.187024\n",
       "  219    32.029279\n",
       "  220    38.139158\n",
       "  221    40.421018\n",
       "  222    34.882659\n",
       "  223    40.286867\n",
       "  224    52.148154\n",
       "  225    47.236697\n",
       "  226    33.450378\n",
       "  227    37.450387\n",
       "  dtype: float64, 'air_735bcbe1763d6e98.csv': 172     7.248719\n",
       "  173     8.619597\n",
       "  174    10.082030\n",
       "  175    11.413089\n",
       "  176    17.167754\n",
       "  177    18.302528\n",
       "  178     8.259196\n",
       "  179     9.634534\n",
       "  180    12.333774\n",
       "  181    10.855221\n",
       "  182    23.875207\n",
       "  183    16.768097\n",
       "  184    10.819550\n",
       "  185     9.299937\n",
       "  186    15.050571\n",
       "  187    11.828718\n",
       "  188    20.072877\n",
       "  189    17.573438\n",
       "  190     9.676733\n",
       "  191    15.508248\n",
       "  192    12.092852\n",
       "  193    20.986832\n",
       "  194    18.417980\n",
       "  195    10.589643\n",
       "  196    12.250434\n",
       "  dtype: float64, 'air_8093d0b565e9dbdf.csv': 386    37.794787\n",
       "  387    45.143601\n",
       "  388    41.638016\n",
       "  389    59.788664\n",
       "  390    38.913582\n",
       "  391    27.975216\n",
       "  392    33.976819\n",
       "  393    38.382738\n",
       "  394    45.566395\n",
       "  395    42.547981\n",
       "  396    60.050506\n",
       "  397    28.421532\n",
       "  398    27.585181\n",
       "  399    35.409802\n",
       "  400    39.322901\n",
       "  401    48.050309\n",
       "  402    45.915677\n",
       "  403    64.264470\n",
       "  404    41.078844\n",
       "  405    29.850899\n",
       "  406    35.115076\n",
       "  407    40.810172\n",
       "  408    49.115292\n",
       "  409    48.743366\n",
       "  410    63.432047\n",
       "  411    42.853909\n",
       "  412    31.575948\n",
       "  413    37.367914\n",
       "  414    41.032053\n",
       "  dtype: float64, 'air_97b2a9f975fc702c.csv': 173    29.074928\n",
       "  174    34.116789\n",
       "  175    32.212606\n",
       "  176    57.051286\n",
       "  177    68.754522\n",
       "  178    36.105360\n",
       "  179    24.604124\n",
       "  180    31.603910\n",
       "  181    33.371983\n",
       "  182    34.847106\n",
       "  183    69.100758\n",
       "  184    74.402926\n",
       "  185    43.890742\n",
       "  186    26.457005\n",
       "  187    37.171702\n",
       "  188    39.213289\n",
       "  189    37.128576\n",
       "  190    59.253742\n",
       "  191    72.919201\n",
       "  192    45.525807\n",
       "  193    25.240463\n",
       "  194    37.482631\n",
       "  195    36.964484\n",
       "  196    34.098679\n",
       "  197    71.721110\n",
       "  198    77.316147\n",
       "  199    47.812379\n",
       "  200    34.093074\n",
       "  201    42.360699\n",
       "  dtype: float64, 'air_9aa92007e3628dbc.csv': 176    22.614050\n",
       "  177    20.846279\n",
       "  178    18.559569\n",
       "  179    31.726756\n",
       "  180    37.412713\n",
       "  181    21.410153\n",
       "  182    19.189154\n",
       "  183    19.847752\n",
       "  184    18.254764\n",
       "  185    43.984922\n",
       "  186    42.459120\n",
       "  187    21.525767\n",
       "  188    19.884021\n",
       "  189    23.103378\n",
       "  190    13.015950\n",
       "  191    22.554211\n",
       "  192    38.362897\n",
       "  193    20.258277\n",
       "  194    17.368821\n",
       "  195    15.195856\n",
       "  196    13.970483\n",
       "  197    25.315312\n",
       "  198    36.413702\n",
       "  199    17.176905\n",
       "  200    18.077454\n",
       "  dtype: float64, 'air_9c6787aa03a45586.csv': 2017-01-31     1.999999\n",
       "  2017-02-01    10.709280\n",
       "  2017-02-02    12.753702\n",
       "  2017-02-03    28.724355\n",
       "  2017-02-06    27.895066\n",
       "  2017-02-07    26.573243\n",
       "  2017-02-08    34.998482\n",
       "  2017-02-09    28.940002\n",
       "  2017-02-10    42.283006\n",
       "  2017-02-13    43.467523\n",
       "  2017-02-14    45.077044\n",
       "  2017-02-15    52.098353\n",
       "  2017-02-16    53.739576\n",
       "  2017-02-17    68.899995\n",
       "  2017-02-20    57.751686\n",
       "  2017-02-21    61.839726\n",
       "  2017-02-22    52.692718\n",
       "  2017-02-23    59.337164\n",
       "  2017-02-24    61.364132\n",
       "  2017-02-27    60.390752\n",
       "  2017-02-28    62.354191\n",
       "  2017-03-01    50.947744\n",
       "  2017-03-02    50.170433\n",
       "  Freq: B, dtype: float64, 'air_b45b8e456f53942a.csv': 188     9.365806\n",
       "  189     8.963036\n",
       "  190     9.910821\n",
       "  191    13.265664\n",
       "  192    19.640816\n",
       "  193    13.392889\n",
       "  194    15.530100\n",
       "  195    11.739430\n",
       "  196    16.697810\n",
       "  197    25.284243\n",
       "  198    14.327857\n",
       "  199    16.129249\n",
       "  200    13.970208\n",
       "  201    12.760883\n",
       "  202    12.120234\n",
       "  203    18.825388\n",
       "  204    10.355895\n",
       "  205    15.026638\n",
       "  206    17.089076\n",
       "  207    16.451070\n",
       "  208    20.342517\n",
       "  209    20.664029\n",
       "  210    10.906212\n",
       "  211    11.005365\n",
       "  212    15.066572\n",
       "  dtype: float64, 'air_c3585b0fba3998d0.csv': 149    10.240093\n",
       "  150    13.703811\n",
       "  151     4.907797\n",
       "  152    15.945782\n",
       "  153    19.247928\n",
       "  154    10.027621\n",
       "  155     8.363757\n",
       "  156    14.276984\n",
       "  157    10.356797\n",
       "  158     4.379741\n",
       "  159     8.467382\n",
       "  160     5.730036\n",
       "  161    14.286285\n",
       "  162    14.025760\n",
       "  163    10.272596\n",
       "  dtype: float64, 'air_caf996ac27206301.csv': 186    2.666507\n",
       "  187    2.217062\n",
       "  188    3.897525\n",
       "  189    4.210876\n",
       "  190    5.058862\n",
       "  191    4.472371\n",
       "  192    2.742560\n",
       "  193    2.019505\n",
       "  194    3.277532\n",
       "  195    3.221548\n",
       "  196    7.397180\n",
       "  197    4.468123\n",
       "  198    1.767838\n",
       "  199    2.915257\n",
       "  200    4.990171\n",
       "  201    5.291964\n",
       "  202    3.779385\n",
       "  203    3.459755\n",
       "  204    0.698403\n",
       "  205    4.850687\n",
       "  206    2.633169\n",
       "  207    5.853670\n",
       "  208    4.760804\n",
       "  209    1.283232\n",
       "  210    0.851440\n",
       "  dtype: float64, 'air_d4d218b451f82c3d.csv': 200    13.933934\n",
       "  201    15.925403\n",
       "  202    14.091373\n",
       "  203    17.441987\n",
       "  204     8.805464\n",
       "  205    12.950601\n",
       "  206    13.699961\n",
       "  207    16.135238\n",
       "  208    12.603672\n",
       "  209    15.948793\n",
       "  210     3.407820\n",
       "  211    12.731265\n",
       "  212    14.638865\n",
       "  213    16.810255\n",
       "  214    15.035900\n",
       "  215    17.449845\n",
       "  216     8.291515\n",
       "  217     7.289706\n",
       "  218    12.408559\n",
       "  219    15.982930\n",
       "  220    15.014253\n",
       "  221    16.188601\n",
       "  222    15.255030\n",
       "  223    10.665661\n",
       "  224     6.820024\n",
       "  225    12.442738\n",
       "  226    15.347144\n",
       "  dtype: float64, 'air_dea0655f96947922.csv': 390    19.146491\n",
       "  391    31.421351\n",
       "  392    26.094405\n",
       "  393    29.537000\n",
       "  394    43.225891\n",
       "  395    35.358279\n",
       "  396    20.894280\n",
       "  397    21.579448\n",
       "  398    29.393428\n",
       "  399    22.476141\n",
       "  400    31.266968\n",
       "  401    55.017759\n",
       "  402    38.630245\n",
       "  403    19.708710\n",
       "  404    21.364104\n",
       "  405    30.480103\n",
       "  406    26.199541\n",
       "  407    28.135125\n",
       "  408    38.210934\n",
       "  409    40.197769\n",
       "  410    14.745157\n",
       "  411    20.775011\n",
       "  412    30.305434\n",
       "  413    21.091103\n",
       "  414    28.708679\n",
       "  415    42.046778\n",
       "  416    39.059262\n",
       "  417    19.744723\n",
       "  418    22.328932\n",
       "  dtype: float64})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.bag_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'air_35512c42db0868da.csv': [0.9050488850149555, 14.803595513455448],\n",
       " 'air_399904bdb7685ca0.csv': [0.8190052639915087, 408.55234115709425],\n",
       " 'air_3bb99a1fe0583897.csv': [0.697097741333219, 292.77662859933423],\n",
       " 'air_465bddfed3353b23.csv': [0.5138629106134974, 225.29325866378471],\n",
       " 'air_56ea46c14b2dd967.csv': [0.8074954867790721, 195.98230681769328],\n",
       " 'air_735bcbe1763d6e98.csv': [2.805219513362827, 82.309830539164423],\n",
       " 'air_8093d0b565e9dbdf.csv': [0.3948572576585378, 196.66957268840667],\n",
       " 'air_97b2a9f975fc702c.csv': [0.5454366869689788, 323.08739885427008],\n",
       " 'air_9aa92007e3628dbc.csv': [0.501445254960189, 367.05581942925335],\n",
       " 'air_9c6787aa03a45586.csv': [0.6910849341954948, 915.29350433938771],\n",
       " 'air_b45b8e456f53942a.csv': [1.6961959053274809, 92.446616956130214],\n",
       " 'air_c3585b0fba3998d0.csv': [1.808139008541642, 110.64072580783402],\n",
       " 'air_caf996ac27206301.csv': [0.7708130735324437, 13.405655818394575],\n",
       " 'air_d4d218b451f82c3d.csv': [1.831434940349982, 54.230373273943975],\n",
       " 'air_dea0655f96947922.csv': [0.31139357005533325, 222.09926236271386]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.bag_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_losses(param_grid=None,rest_dir=\"arima_rest_data/train/\",\\\n",
    "              num_rests=10, len_rests=665):\n",
    "    ## initialize grid_model_params dict\n",
    "    ## keys: (order param lists tuples in the grid)\n",
    "    ## vals: {dicts with keys=rest_ids: values=corresponding preds}  \n",
    "    grid_model_preds = {}\n",
    "    ## for each parameter tuple in the passed grid\n",
    "    for params in param_grid:\n",
    "        print (params)\n",
    "        ## initialize an empty dict\n",
    "        grid_model_preds[params] = {}\n",
    "        ## select \"num_rests\" number of restaurant ids from the directory\n",
    "        for rest in [os.listdir(rest_dir)[1:][ix] for ix in \\\n",
    "                 [np.random.choice(len_rests) for x in range(num_rests)]]:\n",
    "            ## the training data of the corresponding rest_id\n",
    "            print (rest)\n",
    "            if \"total_area_visitors\" not in train_dict[rest].columns.values or\\\n",
    "            \"total_genre_visitors\" not in train_dict[rest].columns.values:\n",
    "                None\n",
    "            elif train_dict[rest].shape[0] < 24 and val_dict[rest].shape[0] < 8:\n",
    "                None\n",
    "            else:    \n",
    "                data = train_dict[rest]\n",
    "                ## the validation data of the corresponding rest_id\n",
    "                val_data = val_dict[rest]\n",
    "                ## SARIMAX model for the current parameters and data\n",
    "                sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                               order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                        seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                        int(params[5]),int(params[6])),\\\n",
    "                                        trend=params[7], mle_regression=True, \\\n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "                results_object = sarimax_model.fit(maxiter=1000,method=\"bfgs\")\n",
    "                ## predictions: len(preds) = val_data.shape[0]\n",
    "                \n",
    "                sarimax_model1 = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "                               order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "                                        seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "                                                        int(params[5]),int(params[6])),\\\n",
    "                                        trend=params[7], mle_regression=True, \\\n",
    "                                        enforce_invertibility=False)\n",
    "                results_object1 = sarimax_model1.fit(start_params=results_object.params,\\\n",
    "                                                    maxiter=1000, method=\"bfgs\")\n",
    "                \n",
    "                predictions_ = results_object1.predict\\\n",
    "                (start=len(data)-1, end=len(data)+len(val_data)-1,dynamic=True,\\\n",
    "                exog=val_data[exog_col].head(2))\n",
    "                \n",
    "                pred = np.array(predictions_)\n",
    "                true = np.array(val_data[endog_col])\n",
    "                \n",
    "                abs_percent_loss = loss_mape(true,pred)\n",
    "                print (abs_percent_loss)\n",
    "                square_loss = loss_squared(true,pred)\n",
    "                print (square_loss)\n",
    "                grid_model_preds[params][rest] = [abs_percent_loss,square_loss]\n",
    "\n",
    "    return grid_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter_num in list(grid_search_iters.keys()): \n",
    "    iter_data = pd.DataFrame(grid_search_iters[iter_num])\n",
    "    iter_data.to_csv(\"grid_search_preds/\" + str(iter_num) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = (1,1,1,1,1,1,1,\"c\")\n",
    "# sarimax_model = SARIMAX(endog=data[endog_col],exog=data[exog_col],\\\n",
    "#                                order=(int(params[0]),int(params[1]),int(params[2])),\\\n",
    "#                                         seasonal_order=(int(params[3]),int(params[4]),\\\n",
    "#                                                         int(params[5]),int(params[6])),\\\n",
    "#                                         trend=params[7], mle_regression=True, \\\n",
    "#                                         enforce_invertibility=False)\n",
    "# results_object1 = sarimax_model.fit(maxiter=1000,method=\"bfgs\",\\\n",
    "#                                          return_params=False)\n",
    "# results_object1.plot_diagnostics(figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02812093,  0.07614396,  0.28566933,  0.98531615,  1.18059439,\n",
       "        0.02814182,  0.04082045,  0.06383652,  0.74416884,  0.28821706,\n",
       "        0.66639737,  1.45516273,  0.01043705,  0.37161201,  0.45667246,\n",
       "        0.51899032,  0.23114242,  0.4349709 ,  7.82004663,  0.6566238 ,\n",
       "        0.17851903,  0.07183971,  0.11575361,  0.49033753])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(np.abs(np.subtract(true[:24],pred[:24])), np.abs(true[:24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in grid: \n",
    "    if param in list(grid_model_params.keys()):\n",
    "        param_data = pd.DataFrame(grid_model_params[param])\n",
    "        param_data.to_csv(\"param_grid_csv/\" + str(param) + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
